{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, we will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. We will use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, we will apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that we will use has been provided by Bertelsmann Arvato Analytics, and represents a real-life data science task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. We use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use our analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "azdias = pd.read_csv('./Data/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "customers = pd.read_csv('./Data/Udacity_CUSTOMERS_052018.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check the categorical features for azdias and if they need to be corrected\n",
    "cat_columns_azdias = azdias.select_dtypes(include='object').columns\n",
    "for cat in cat_columns_azdias:\n",
    "    print('for column {} there are {} unique elements'.format(cat,len(azdias.loc[:,cat].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check the categorical features for customers and if they need to be corrected\n",
    "cat_columns_customers = customers.select_dtypes(include='object').columns\n",
    "for cat in cat_columns_customers:\n",
    "    print('for column {} there are {} unique elements'.format(cat,len(customers.loc[:,cat].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we understand that EINGEFUEGT_AM column in both datasets includes specific dates. So it should be converted to numbers\n",
    "# this way we then remove this column from categorical features that can hugely reduce the number of features\n",
    "# after using get_dummies() method\n",
    "azdias['EINGEFUEGT_AM'] = pd.to_datetime(azdias['EINGEFUEGT_AM'])\n",
    "azdias['EINGEFUEGT_AM'] = azdias.loc[~azdias['EINGEFUEGT_AM'].isnull(),'EINGEFUEGT_AM'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for customers\n",
    "customers['EINGEFUEGT_AM'] = pd.to_datetime(customers['EINGEFUEGT_AM'])\n",
    "customers['EINGEFUEGT_AM'] = customers.loc[~customers['EINGEFUEGT_AM'].isnull(),'EINGEFUEGT_AM'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we check the categorical variables in azdias and customers\n",
    "print('--------------azdias-----------')\n",
    "cat_columns_azdias = azdias.select_dtypes(include='object').columns\n",
    "for cat in cat_columns_azdias:\n",
    "    print('for column {} there are {} unique elements'.format(cat,len(azdias.loc[:,cat].unique())))\n",
    "print('--------------customers-----------')\n",
    "cat_columns_customers = customers.select_dtypes(include='object').columns\n",
    "for cat in cat_columns_customers:\n",
    "    print('for column {} there are {} unique elements'.format(cat,len(customers.loc[:,cat].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that the problem with EINGEFUEGT_AM column is solved. Now we check for further problems in categorical columns\n",
    "# we see that we have problem with CAMEO_DEUG_2015 and CAMEO_INTL_2015 columns. Records are int numbers but saved as objects\n",
    "# we then convert them to numbers\n",
    "# we convert 'X' values to -1 as unknown\n",
    "azdias.loc[azdias['CAMEO_DEUG_2015']=='X','CAMEO_DEUG_2015']=-1\n",
    "# we then convert values other than nan to int values\n",
    "azdias['CAMEO_DEUG_2015'] = azdias.loc[~azdias['CAMEO_DEUG_2015'].isnull(),'CAMEO_DEUG_2015'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for CAMEO_INTL_2015 column\n",
    "azdias.loc[azdias['CAMEO_INTL_2015']=='XX','CAMEO_INTL_2015']=-1\n",
    "azdias['CAMEO_INTL_2015'] = azdias.loc[~azdias['CAMEO_INTL_2015'].isnull(),'CAMEO_INTL_2015'].astype(int)\n",
    "# do it for customers\n",
    "customers.loc[customers['CAMEO_DEUG_2015']=='X','CAMEO_DEUG_2015']=-1\n",
    "customers['CAMEO_DEUG_2015'] = customers.loc[~customers['CAMEO_DEUG_2015'].isnull(),'CAMEO_DEUG_2015'].astype(int)\n",
    "customers.loc[customers['CAMEO_INTL_2015']=='XX','CAMEO_INTL_2015']=-1\n",
    "customers['CAMEO_INTL_2015'] = customers.loc[~customers['CAMEO_INTL_2015'].isnull(),'CAMEO_INTL_2015'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check for categorical features\n",
    "print('--------------azdias-----------')\n",
    "cat_columns_azdias = azdias.select_dtypes(include='object').columns\n",
    "for cat in cat_columns_azdias:\n",
    "    print('for column {} there are {} unique elements'.format(cat,len(azdias.loc[:,cat].unique())))\n",
    "print('--------------customers-----------')\n",
    "cat_columns_customers = customers.select_dtypes(include='object').columns\n",
    "for cat in cat_columns_customers:\n",
    "    print('for column {} there are {} unique elements'.format(cat,len(customers.loc[:,cat].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we try to wrangle azdias and prepare it to start our analysis\n",
    "# calculating null percentage of each column to decide which column to be eliminated\n",
    "for i in range(azdias.shape[1]):\n",
    "    print('Null percentage for column {} is {:.1f}%'.format(i,(azdias.iloc[:,i].isnull().sum()/azdias.shape[0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that column 300 is relevant to cutomer status which is important for our modelling\n",
    "# while 65.6% of this column is null. We then decide to change all nan values to -1 which means unknown\n",
    "azdias.iloc[:,300].fillna(-1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column 100 is relevant to EXTSEL992 key that has no definition in the excel file with 73.4% null vlues\n",
    "# we then think that this column is not important for the modelling and drop whole the column\n",
    "azdias.drop(['EXTSEL992'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step is to drop all columns with more than 90 percent of null data as imputing\n",
    "# would not reflect the precise action for these columns. \n",
    "for i in range(azdias.shape[1]):\n",
    "    if(azdias.iloc[:,i].isnull().sum()/azdias.shape[0]*100>90):\n",
    "        print('Null percentage more than 90% \\\n",
    "        for column {} with null percentage {:.1f}%'.\\\n",
    "              format(i,(azdias.iloc[:,i].isnull().sum()/\\\n",
    "                        azdias.shape[0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then decide to eliminate each column with more than 90% null data\n",
    "dropped_cols = azdias.columns[4:8]\n",
    "azdias.drop(dropped_cols,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is one record with more than 90% null data. We proceed with dropping this record and \n",
    "# continue with imputing null records for all the columns\n",
    "for i in range(azdias.shape[0]):\n",
    "    if((azdias.iloc[i,:].isnull().sum()/azdias.shape[1])*100>90):\n",
    "        print('Record {} has more than 90% null columns with null\\\n",
    "        percent {:.1f}%'.format(i,(azdias.iloc[i,:].isnull().sum()/azdias.\\\n",
    "                                   shape[1])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result is no record having more than 90% of null data. We thus continue with imputing null records \n",
    "# for all the remaining columns. We choose mode of each column to replace null values as we are working \n",
    "# with categorical data for all the columns.\n",
    "for col in azdias.columns:\n",
    "    try:\n",
    "        azdias[col] = azdias[col].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    except:\n",
    "        print('That broke...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking null values\n",
    "for i in range(azdias.shape[1]):\n",
    "    print('Null percentage for column {} is {:.1f}%'.format(i,(azdias.iloc[:,i].isnull().sum()/azdias.shape[0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do the same data wrangling for customers dataframe\n",
    "# First we try to wrangle customers df and prepare it to start our analysis\n",
    "# calculating null percentage of each column to decide which column to be eliminated\n",
    "for i in range(customers.shape[1]):\n",
    "    print('Null percentage for column {} is {:.1f}%'.format(i,(customers.iloc[:,i].isnull().sum()/customers.shape[0])*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we decide to eliminate each column with more than 90% null data\n",
    "dropped_cols = customers.columns[4:8]\n",
    "customers.drop(dropped_cols,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all the remaining columns. We choose mode of each column to replace null values as we are working \n",
    "# with categorical data for all the columns.\n",
    "for col in customers.columns:\n",
    "    try:\n",
    "        customers[col] = customers[col].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    except:\n",
    "        print('That broke...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking null values\n",
    "for i in range(customers.shape[1]):\n",
    "    print('Null percentage for column {} is {:.1f}%'.format(i,(customers.iloc[:,i].isnull().sum()/customers.shape[0])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of our analysis will come in this part of the project. Here, we use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, we would be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the algorithm: we first prepare datasets as they have new dummy variables. \n",
    "# Then dimensional reduction will be done for two datasets, considering elbow method to find the optimum number of features\n",
    "# to take up 90% of the total variance of the original data. Then two datasets will be standardized and clustered (again considering\n",
    "# elbow method to find optimum number of clusters) using K-means algorithm. The cluster labels then will be compared to find the \n",
    "# similarity between clusters employing some specific metrics. The final outcome would be the clusters in two datasets that are mostly \n",
    "# similar and the members in two different datasets can represent eachother.\n",
    "\n",
    "\n",
    "# Step 1\n",
    "# add dummy variables to customers and azdias datasets\n",
    "customers = pd.get_dummies(customers)\n",
    "azdias = pd.get_dummies(azdias)\n",
    "# write to drive\n",
    "customers.to_csv('./Data/customers_filled.csv',index=False)\n",
    "azdias.to_csv('./Data/azdias_filled.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 30 percent of data as it is hyper-dimensional and huge in number of records\n",
    "customers = pd.read_csv('./Data/customers_filled.csv')\n",
    "azdias = pd.read_csv('./Data/azdias_filled.csv')\n",
    "\n",
    "# sampling original processed data\n",
    "customers_sample = customers.sample(frac=0.3).reset_index(drop=True)\n",
    "azdias_sample = azdias.sample(frac=0.3).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customers standardization\n",
    "# Select columns to standardize (excluding the first column)\n",
    "columns_to_standardize = customers_sample.columns[1:]\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data\n",
    "scaler.fit(customers_sample[columns_to_standardize])\n",
    "\n",
    "# Transform the data\n",
    "customers_sample[columns_to_standardize] = scaler.transform(customers_sample[columns_to_standardize])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azdias standardization\n",
    "# Select columns to standardize (excluding the first column)\n",
    "columns_to_standardize = azdias_sample.columns[1:]\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data\n",
    "scaler.fit(azdias_sample[columns_to_standardize])\n",
    "\n",
    "# Transform the data\n",
    "azdias_sample[columns_to_standardize] = scaler.transform(azdias_sample[columns_to_standardize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy to new dataframes\n",
    "azdias=azdias_sample.copy()\n",
    "customers=customers_sample.copy()\n",
    "# write to drive\n",
    "azdias.to_csv('./Data/azdias_std_sample.csv',index=False)\n",
    "customers.to_csv('./Data/customers_std_sample.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read sample data\n",
    "azdias=pd.read_csv('./Data/azdias_std_sample.csv')\n",
    "customers=pd.read_csv('./Data/customers_std_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# Now we proceed with dimensional reduction to a COMMON number of features that can fill up about 90% of the original data variance\n",
    "# we first try to find the optimum number of features for both datasets to conduct dimensional reduction\n",
    "\n",
    "# find the common features between the two datasets\n",
    "common_features = list(set(azdias.columns) & set(customers.columns))\n",
    "# remove identifier column\n",
    "common_features.remove('LNR')\n",
    "# concatenate two datasets considering common features  \n",
    "combined_df = pd.concat([azdias[common_features], customers[common_features]], ignore_index=True)\n",
    "\n",
    "best_components = 0  # Variable to store the best number of components\n",
    "max_features = combined_df.shape[1]\n",
    "\n",
    "for num_components in range(1, max_features + 1):\n",
    "    pca = PCA(n_components=num_components)\n",
    "    df_reduced = pca.fit_transform(combined_df)\n",
    "    \n",
    "    explained_variance = sum(pca.explained_variance_ratio_)\n",
    "    print('for number of features={}, explained variance ratio is {}'.format(num_components,explained_variance))\n",
    "\n",
    "    if explained_variance > 0.9:\n",
    "        \n",
    "        best_components = num_components\n",
    "        break\n",
    "\n",
    "print('Best number of features that can explain 90 percent of the variance is: ',best_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we found that ncomponents=232 can explain 90% of the variance in the original combined dataset\n",
    "# we then proceed with doing dimensional reduction for azdias and customers with this number of\n",
    "# components\n",
    "ncomponents=232\n",
    "# azdias dimensional reduction\n",
    "pca = PCA(n_components=ncomponents)\n",
    "df_reduced1 = pca.fit_transform(azdias.iloc[:,1:])\n",
    "azdias_reduced = pd.concat([azdias.iloc[:,0],pd.DataFrame(df_reduced1)],axis=1)\n",
    "\n",
    "# customers dimensional reduction\n",
    "pca = PCA(n_components=ncomponents)\n",
    "df_reduced2 = pca.fit_transform(customers.iloc[:,1:])\n",
    "customers_reduced = pd.concat([customers.iloc[:,0],pd.DataFrame(df_reduced2)],axis=1)\n",
    "\n",
    "# write to drive\n",
    "azdias_reduced.to_csv('./Data/azdias_reduced.csv',index=False)\n",
    "customers_reduced.to_csv('./Data/customers_reduced.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read reduce data\n",
    "azdias_reduced = pd.read_csv('./Data/azdias_reduced.csv')\n",
    "customer_reduced = pd.read_csv('./Data/customers_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proceed with implementing elbow method to find the best cluster number for azdias dataset\n",
    "# kmeans.inertia_ is sum of squared distances of samples to their closest cluster center. We are looking \n",
    "# for the place where this number would not change drastically by increasing the number of clusters (elbow).\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 20):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++')\n",
    "    kmeans.fit(azdias_reduced.iloc[:,1:].values)\n",
    "    wcss.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result\n",
    "plt.plot(range(1, 20), wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters for demography data')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the plot, we see that n_clusters=10 would be a good choice for clustering as inertia \n",
    "# is not meaningfully changing after this number. We thus go forward to cluster azdias dataset with\n",
    "# n_clusters=10 and consider 1 cluster for customer dataset\n",
    "\n",
    "# azdias clustering\n",
    "kmeans1 = KMeans(n_clusters = 10, init = 'k-means++')\n",
    "azdias_kmeans = kmeans1.fit_predict(azdias_reduced.iloc[:,1:])\n",
    "\n",
    "# customers clustering\n",
    "kmeans2 = KMeans(n_clusters = 1, init = 'k-means++')\n",
    "customer_kmeans = kmeans2.fit_predict(customer_reduced.iloc[:,1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# calculate cluster similarity by calculating Euclidean pairwise distance between clusters in two datasets\n",
    "centroid_distances = pairwise_distances(kmeans1.cluster_centers_, kmeans2.cluster_centers_)\n",
    "# convert Euclidean distances to similarities (using inverse distance)\n",
    "similarity_matrix = 1 / (1 + centroid_distances)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix\n",
    "# we choose clusters 3 and 8 of the demography as the most similar clusters to the company customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it means that clusters 3 and 8 from azdias are most similar to customers of the company\n",
    "\n",
    "# assigning cluster labels to each dataset\n",
    "azdias = azdias_reduced.copy()\n",
    "customer = customer_reduced.copy()\n",
    "azdias['Cluster_label']=kmeans1.labels_\n",
    "customer['Cluster_label']=kmeans2.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have now two datasets that are clustered and each cluster from azdias is mapped to the cluster in customer\n",
    "# we make a dictionary including all individuals that are most likely similar to company customers\n",
    "\n",
    "affine_individuals = {'Similar Individuals':[]}\n",
    "for i in (3,8):\n",
    "    affine_individuals['Similar Individuals'].extend(azdias.loc[azdias['Cluster_label']==i,'LNR'].values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of individuals in the demography data that \\\n",
    "     \\ncan represent customers in the company are {}'.format(len(affine_individuals['Similar Individuals'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary using pickling\n",
    "import pickle\n",
    "\n",
    "with open('affinity.pickle', 'wb') as file:\n",
    "    pickle.dump(affine_individuals, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that we've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, we can verify our model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, we'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this part, we train and verify our model based on Udacity_MAILOUT_052018_TRAIN.csv dataset\n",
    "train_dataset = pd.read_csv('./Data/Udacity_MAILOUT_052018_TRAIN.csv',sep=';')\n",
    "\n",
    "# preprocess train data\n",
    "# we check the categorical features for train dataset and if they need to be corrected\n",
    "cat_columns = train_dataset.select_dtypes(include='object').columns\n",
    "for cat in cat_columns:\n",
    "    print('for column {} there are {} unique elements'.format(cat,len(train_dataset.loc[:,cat].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we understand that EINGEFUEGT_AM column in train dataset includes specific dates. So it should be converted to numbers\n",
    "# this way we then remove this column from categorical features that can hugely reduce the number of features\n",
    "# after using get_dummies() method\n",
    "train_dataset['EINGEFUEGT_AM'] = pd.to_datetime(train_dataset['EINGEFUEGT_AM'])\n",
    "train_dataset['EINGEFUEGT_AM'] = train_dataset.loc[~train_dataset['EINGEFUEGT_AM'].isnull(),'EINGEFUEGT_AM'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we check the categorical variables in train dataset \n",
    "print('--------------Train dataset-----------')\n",
    "cat_columns = train_dataset.select_dtypes(include='object').columns\n",
    "for cat in cat_columns:\n",
    "    print('for column {} there are {} unique elements'.format(cat,len(train_dataset.loc[:,cat].unique())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that the problem with EINGEFUEGT_AM column is solved. Now we check for further problems in categorical columns\n",
    "# we see that we have problem with CAMEO_DEUG_2015 and CAMEO_INTL_2015 columns. Records are int numbers but saved as objects\n",
    "# we then convert them to numbers\n",
    "# we convert 'X' values to -1 as unknown\n",
    "train_dataset.loc[train_dataset['CAMEO_DEUG_2015']=='X','CAMEO_DEUG_2015']=-1\n",
    "# we then convert values other than nan to int values\n",
    "train_dataset['CAMEO_DEUG_2015'] = train_dataset.loc[~train_dataset['CAMEO_DEUG_2015'].isnull(),'CAMEO_DEUG_2015'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for CAMEO_INTL_2015 column\n",
    "train_dataset.loc[train_dataset['CAMEO_INTL_2015']=='XX','CAMEO_INTL_2015']=-1\n",
    "train_dataset['CAMEO_INTL_2015'] = train_dataset.loc[~train_dataset['CAMEO_INTL_2015'].isnull(),'CAMEO_INTL_2015'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check for categorical features\n",
    "print('--------------Train dataset-----------')\n",
    "cat_columns = train_dataset.select_dtypes(include='object').columns\n",
    "for cat in cat_columns:\n",
    "    print('for column {} there are {} unique elements'.format(cat,len(train_dataset.loc[:,cat].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate null percentage of each column to decide which column to be eliminated\n",
    "for i in range(train_dataset.shape[1]):\n",
    "    print('Null percentage for column {} is {:.1f}%'.format(i,(train_dataset.iloc[:,i].isnull().sum()/train_dataset.shape[0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that column 300 is relevant to cutomer status which is important for our modelling\n",
    "# while 65.6% of this column is null. We then decide to change all nan values to -1 which means unknown\n",
    "train_dataset.iloc[:,300].fillna(-1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step is to drop all columns with more than 90 percent of null data as imputing\n",
    "# would not reflect the precise action for these columns. \n",
    "for i in range(train_dataset.shape[1]):\n",
    "    if(train_dataset.iloc[:,i].isnull().sum()/train_dataset.shape[0]*100>90):\n",
    "        print('Null percentage more than 90% \\\n",
    "        for column {} with null percentage {:.1f}%'.\\\n",
    "              format(i,(train_dataset.iloc[:,i].isnull().sum()/\\\n",
    "                        train_dataset.shape[0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then decide to eliminate each column with more than 90% null data\n",
    "dropped_cols = train_dataset.columns[4:8]\n",
    "train_dataset.drop(dropped_cols,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is one record with more than 90% null data. We proceed with dropping this record and \n",
    "# continue with imputing null records for all the columns\n",
    "for i in range(train_dataset.shape[0]):\n",
    "    if((train_dataset.iloc[i,:].isnull().sum()/train_dataset.shape[1])*100>90):\n",
    "        print('Record {} has more than 90% null columns with null\\\n",
    "        percent {:.1f}%'.format(i,(train_dataset.iloc[i,:].isnull().sum()/train_dataset.\\\n",
    "                                   shape[1])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result is no record having more than 90% of null data. We thus continue with imputing null records \n",
    "# for all the remaining columns. We choose mode of each column to replace null values as we are working \n",
    "# with categorical data for all the columns.\n",
    "for col in train_dataset.columns:\n",
    "    try:\n",
    "        train_dataset[col] = train_dataset[col].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    except:\n",
    "        print('That broke...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values\n",
    "for i in range(train_dataset.shape[1]):\n",
    "    print('Null percentage for column {} is {:.1f}%'.format(i,(train_dataset.iloc[:,i].isnull().sum()/train_dataset.shape[0])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with categorical varibles\n",
    "train_dataset = pd.get_dummies(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now preprocessing is done and we can start training the model with train dataset\n",
    "# and employing random forest algorithm\n",
    "X = train_dataset.drop('RESPONSE',axis=1).values\n",
    "y = train_dataset.loc[:, 'RESPONSE'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset: 75% train data, 25% test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the response for test set\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that only 123 responses are incorrectly predicted and accuracy is 98.8 %\n",
    "# this verifies the credibility of the trained mode. Last part of the project is to\n",
    "# employ this trained model to predict response of Udacity_MAILOUT_052018_TEST.csv\n",
    "import pickle\n",
    "\n",
    "with open('model.pickle', 'wb') as file:\n",
    "    pickle.dump(classifier, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "test_dataset = pd.read_csv('./Data/Udacity_MAILOUT_052018_TEST.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do the same preprocessing as for the training dataset\n",
    "test_dataset['EINGEFUEGT_AM'] = pd.to_datetime(test_dataset['EINGEFUEGT_AM'])\n",
    "test_dataset['EINGEFUEGT_AM'] = test_dataset.loc[~test_dataset['EINGEFUEGT_AM'].isnull(),'EINGEFUEGT_AM'].astype(int)\n",
    "test_dataset.loc[test_dataset['CAMEO_INTL_2015']=='XX','CAMEO_INTL_2015']=-1\n",
    "test_dataset['CAMEO_INTL_2015'] = test_dataset.loc[~test_dataset['CAMEO_INTL_2015'].isnull(),'CAMEO_INTL_2015'].astype(int)\n",
    "test_dataset.loc[test_dataset['CAMEO_DEUG_2015']=='X','CAMEO_DEUG_2015']=-1\n",
    "test_dataset['CAMEO_DEUG_2015'] = test_dataset.loc[~test_dataset['CAMEO_DEUG_2015'].isnull(),'CAMEO_DEUG_2015'].astype(int)\n",
    "test_dataset.iloc[:,300].fillna(-1,inplace=True)\n",
    "dropped_cols = test_dataset.columns[4:8]\n",
    "test_dataset.drop(dropped_cols,axis=1,inplace=True)\n",
    "for col in test_dataset.columns:\n",
    "    try:\n",
    "        test_dataset[col] = test_dataset[col].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "    except:\n",
    "        print('That broke...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.get_dummies(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(test_dataset.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the response for test set\n",
    "y = classifier.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add RESPONSE column to test dataset\n",
    "labeled_test_data = pd.read_csv('./Data/Udacity_MAILOUT_052018_TEST.csv', sep=';')\n",
    "labeled_test_data['RESPONSE'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new labled test file as a csv file\n",
    "labeled_test_data.to_csv('./Data/Udacity_MAILOUT_052018_TEST_LABELED.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for RESPONSE=1 in the labeled data\n",
    "potential_customers = labeled_test_data.loc[labeled_test_data.iloc[:,-1]==1,:].LNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that our trained model predicts 39 individuals would become customer of the company given the \n",
    "# demographic features\n",
    "potential_customers.to_csv('./potential_customers.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
